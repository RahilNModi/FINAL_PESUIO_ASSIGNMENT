{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required models \n",
    "import bs4 as b\n",
    "import lxml\n",
    "import urllib.request\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cream=urllib.request.urlopen('https://karki23.github.io/Weather-Data/assignment.html').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the data to organise it.\n",
    "object_code=b.BeautifulSoup(cream,'lxml')\n",
    "oc=object_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL ASSIGNMENT\n"
     ]
    }
   ],
   "source": [
    "#Initially the title is:\n",
    "print(oc.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albury.html', 'BadgerysCreek.html', 'Cobar.html', 'CoffsHarbour.html', 'Moree.html', 'Newcastle.html', 'NorahHead.html', 'NorfolkIsland.html', 'Penrith.html', 'Richmond.html', 'Sydney.html', 'SydneyAirport.html', 'WaggaWagga.html', 'Williamtown.html', 'Wollongong.html', 'Canberra.html', 'Tuggeranong.html', 'MountGinini.html', 'Ballarat.html', 'Bendigo.html', 'Sale.html', 'MelbourneAirport.html', 'Melbourne.html', 'Mildura.html', 'Nhil.html', 'Portland.html', 'Watsonia.html', 'Dartmoor.html', 'Brisbane.html', 'Cairns.html', 'GoldCoast.html', 'Townsville.html', 'Adelaide.html', 'MountGambier.html', 'Nuriootpa.html', 'Woomera.html', 'Albany.html', 'Witchcliffe.html', 'PearceRAAF.html', 'PerthAirport.html', 'Perth.html', 'SalmonGums.html', 'Walpole.html', 'Hobart.html', 'Launceston.html', 'AliceSprings.html', 'Darwin.html', 'Katherine.html', 'Uluru.html']\n"
     ]
    }
   ],
   "source": [
    "#We require each hyperlink from assignment.html\n",
    "websites=[]\n",
    "links=oc.find_all('a')\n",
    "for i in links:\n",
    "    websites.append(i.get('href'))\n",
    "print(websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is to scrape all 49 cities into one csv file named cities.csv and csv file is in dataset folder\n",
    "#cities.csv dataset is used further everywhere\n",
    "for name in websites:\n",
    "    c1=urllib.request.urlopen('https://karki23.github.io/Weather-Data/'+ name).read()\n",
    "    code1=b.BeautifulSoup(c1,'lxml')\n",
    "    rows=[]\n",
    "    table=code1.find_all('table')\n",
    "    for i in table:\n",
    "        tr=i.find_all('tr')\n",
    "        th=[j.text for j in tr[0].find_all('th')]\n",
    "        rows.append(th)\n",
    "        for k in tr[1:]:\n",
    "            td=[l.text for l in k.find_all('td')]\n",
    "            rows.append(td)\n",
    "    with open('cities.csv','a') as f:\n",
    "        writer=csv.writer(f)\n",
    "        for i in rows:\n",
    "            writer.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#This code is to scrape all 49 cities into different csv files and csv files are in dataset folder\\ncount='1'\\nfor name in websites:\\n    c1=urllib.request.urlopen('https://karki23.github.io/Weather-Data/'+ name).read()\\n    code1=b.BeautifulSoup(c1,'lxml')\\n    rows=[]\\n    table=code1.find_all('table')\\n    for i in table:\\n        tr=i.find_all('tr')\\n        th=[j.text for j in tr[0].find_all('th')]\\n        rows.append(th)\\n        for k in tr[1:]:\\n            td=[l.text for l in k.find_all('td')]\\n            rows.append(td)\\n    with open(str(count)+'.csv','a') as f:\\n        writer=csv.writer(f)\\n        for i in rows:\\n            writer.writerow(i)\\n    count=int(count)+1\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#This code is to scrape all 49 cities into different csv files and csv files are in dataset folder\n",
    "count='1'\n",
    "for name in websites:\n",
    "    c1=urllib.request.urlopen('https://karki23.github.io/Weather-Data/'+ name).read()\n",
    "    code1=b.BeautifulSoup(c1,'lxml')\n",
    "    rows=[]\n",
    "    table=code1.find_all('table')\n",
    "    for i in table:\n",
    "        tr=i.find_all('tr')\n",
    "        th=[j.text for j in tr[0].find_all('th')]\n",
    "        rows.append(th)\n",
    "        for k in tr[1:]:\n",
    "            td=[l.text for l in k.find_all('td')]\n",
    "            rows.append(td)\n",
    "    with open(str(count)+'.csv','a') as f:\n",
    "        writer=csv.writer(f)\n",
    "        for i in rows:\n",
    "            writer.writerow(i)\n",
    "    count=int(count)+1\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
